{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "836658ac-fafe-49ef-a3bc-37030786e87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import display, Image\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image as PilImage\n",
    "import io\n",
    "import time as timer\n",
    "import concurrent.futures\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8de4a90e-2133-4247-b50a-e83cf0ff7225",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_ai_key = open('open_ai_key.txt', 'r').read()\n",
    "os.environ['OPENAI_API_KEY'] = open_ai_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8df6158b-4d0c-4211-b698-529d67a68f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_food_items_gpt(client, img):\n",
    "    \"\"\"Combines label and text detection for better accuracy on food items.\"\"\"\n",
    "    foods = client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      response_format={\"type\": \"json_object\"},\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an image analyzer, skilled at reading images of pantries and identifying every food item present, using object and text detection, and outputting that list in JSON.\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": \"Read and analyze this image of a pantry. Identify all food items using object and text detection. Return a JSON object with a single key, 'food_items', with the value being a list of every detected food item.\"},\n",
    "                            {\"type\": \"image_url\", \"image_url\": {\"url\": img}}\n",
    "                        ]      \n",
    "        }]\n",
    "    )\n",
    "    return foods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42c73ea4-42d1-45f9-a0bf-55ec068a9d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_food_items_gpt_mult(client, imgs):\n",
    "    \"\"\"Combines label and text detection for better accuracy on food items.\"\"\"\n",
    "    msgs = [{\"type\": \"text\", \"text\": \"Read and analyze this image of a pantry. Identify all food items using object and text detection. Return a JSON object with a single key, 'food_items', with the value being a list of every detected food item.\"}]\n",
    "    for img in imgs:\n",
    "        msgs.append({\"type\": \"image_url\", \"image_url\": {\"url\": img}})\n",
    "    foods = client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      response_format={\"type\": \"json_object\"},\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an image analyzer, skilled at reading images of pantries and identifying every food item present, using object and text detection, and outputting that list in JSON.\"},\n",
    "        {\"role\": \"user\", \"content\": msgs    \n",
    "        }]\n",
    "    )\n",
    "    return foods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a296c08-457a-46d6-9df4-2825b474bebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = OpenAI()\n",
    "# start = timer.time()\n",
    "# foods = scan_pantry(client, \"pantry.JPG\")\n",
    "# print(timer.time()-start)\n",
    "# print(foods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "480f7e13-358c-42e7-9dfb-0025e550ea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_pantry(client, img_paths, crop_limit=3):\n",
    "    imgs = []\n",
    "    for img_path in img_paths:\n",
    "        im = PilImage.open(img_path)\n",
    "        width, height = im.size\n",
    "    \n",
    "        # Now crop over the picture and check again with multiple crop sizes\n",
    "        crops = 1\n",
    "        while crops <= crop_limit:\n",
    "            for i in range(crops):\n",
    "                top = max((i/crops-1/(crops*8)),0)*height\n",
    "                bottom = min((9/(crops*8)+i/crops),1)*height\n",
    "                for j in range(crops):\n",
    "                    # crop image\n",
    "                    left = max((j/crops-1/(crops*8)),0)*width\n",
    "                    right = min((9/(crops*8)+j/crops),1)*width\n",
    "                    im_crop = im.crop((left, top, right, bottom))\n",
    "    \n",
    "                    # get url\n",
    "                    buffered = BytesIO()\n",
    "                    im_crop.save(buffered, format=\"JPEG\")\n",
    "                    im_bytes = buffered.getvalue()\n",
    "                    base64_encoded_data = base64.b64encode(im_bytes).decode('utf-8')\n",
    "                    data_url = f\"data:image/jpeg;base64,{base64_encoded_data}\"\n",
    "    \n",
    "                    imgs.append(data_url)\n",
    "            crops+=1\n",
    "\n",
    "    # feed to chatgpt\n",
    "    foods = json.loads(detect_food_items_gpt_mult(client, imgs).choices[0].message.content)['food_items']\n",
    "    return foods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7775ba06-2690-42bb-ae64-f29fd4d77851",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_format = \"The output should have 4 keys: name, time, ingredients, steps. The value of name should be a string with no quotations, underscores, or asterisks. The value of time should be a string with the number and 'minutes' or, if over 60 minutes, the number of hours and minutes combined. The value of ingredients should be a dictionary with the keys being ingredients with no dashes, underscores, quatations, or asterisks and their values being their respective amounts. The value of steps should be a list of all the steps, in order, without numbers.\"\n",
    "def ai_get_recipe_json(client, dish, experience, servings, allergies, pantry):\n",
    "    \"\"\"Fetches a recipe by name and experience level using AI and returns JSON.\"\"\"\n",
    "    if pantry:\n",
    "        recipe = client.chat.completions.create(\n",
    "          model=\"gpt-4o\",\n",
    "          response_format={\"type\": \"json_object\"},\n",
    "          messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a chef, skilled at creating and finding recipes for anyone and anything designed to output JSON.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Create a recipe for {dish} for a person of {experience} experience. Include amounts of each ingredient for {servings} and avoid {allergies} due to allergies. It is known that the person has these ingredients available: {pantry} (there may be a few things that were not scanned, such as certain spices, but expect that most ingredients were scanned). Be sure to inlcude a name and total time. {recipe_format}\"}\n",
    "          ]\n",
    "        ).choices[0].message.content\n",
    "    else:\n",
    "        recipe = client.chat.completions.create(\n",
    "          model=\"gpt-4o\",\n",
    "          response_format={\"type\": \"json_object\"},\n",
    "          messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a chef, skilled at creating and finding recipes for anyone and anything designed to output JSON.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Create a recipe for {dish} for a person of {experience} experience. Include amounts of each ingredient for {servings} and avoid {allergies} due to allergies. Be sure to inlcude the name of the dish and the total time. {recipe_format}\"}\n",
    "          ]\n",
    "        ).choices[0].message.content\n",
    "    return json.loads(recipe)\n",
    "\n",
    "def convert_and_adjust(client, recipe, servings):\n",
    "    \"\"\"Converts existing recipe to JSON using AI.\"\"\"\n",
    "    json_recipe = client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      response_format={\"type\": \"json_object\"},\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a recipe reader, skilled at reading recipes and converting them to JSON.\"},\n",
    "        {\"role\": \"user\", \"content\": recipe},\n",
    "        {\"role\": \"user\", \"content\": f\"Convert that recipe to JSON. Include recipe name, total time, servings, and amounts for each ingredient. Then, modify the amounts of each ingredients for {servings} servings. {recipe_format}\"}\n",
    "      ]\n",
    "    ).choices[0].message.content\n",
    "    return json.loads(json_recipe)\n",
    "\n",
    "def convert_and_adjust_and_modify(client, recipe, servings, pantry):\n",
    "    \"\"\"Converts existing recipe to JSON using AI.\"\"\"\n",
    "    json_recipe = client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      response_format={\"type\": \"json_object\"},\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a recipe reader, skilled at reading recipes and converting them to JSON, then modifying for certain ingredients.\"},\n",
    "        {\"role\": \"user\", \"content\": recipe},\n",
    "        {\"role\": \"user\", \"content\": f\"Convert that recipe to JSON. Include recipe name, total time, servings, and amounts for each ingredient. I may not have all of the ingredients, so you need to modify accordningly. I have these ingredients available: {pantry} (I may have a few others as well, but expect most available items to be listed). Modify the recipe accordingly. Then, modify the amounts of each ingredients for {servings} servings. {recipe_format}\"}\n",
    "      ]\n",
    "    ).choices[0].message.content\n",
    "    return json.loads(json_recipe)\n",
    "\n",
    "def create_grocery_list_ai(client, ingredients, have, executor):\n",
    "    \"\"\"Creates a grocery list for ingredients that user does not have.\"\"\"\n",
    "    global done\n",
    "    # check which ingredients user has\n",
    "    yes = ('y','Y','yes','Yes','YES')\n",
    "    no = ('n','N','no','No','NO')\n",
    "    for i in ingredients:\n",
    "        if i in have:\n",
    "            continue\n",
    "        while True:\n",
    "            check = input(f\"Do you have {ingredients[i]} {i} (y/n)? \")\n",
    "            if check in yes:\n",
    "                have[i] = ingredients[i]\n",
    "                break\n",
    "            elif check in no:\n",
    "                break\n",
    "            print(\"Please enter 'y' or 'n'\")\n",
    "\n",
    "    clear()\n",
    "    # generate grocery list\n",
    "    print(\"Generating grocery list.\", end='')\n",
    "    executor.submit(dots)\n",
    "    list = client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a grocery list reader, skilled at reading multiple lists and synthesizing them to make a new one.\"},\n",
    "        {\"role\": \"assistant\", \"content\": str(ingredients)},\n",
    "        {\"role\": \"user\", \"content\": f\"I have these ingredients: {str(have)}. I need these ingredients: {str(ingredients)}. Make me a grocery list of what I need but don't have, including the amount I should buy of each (1 14 oz can, 2 1lb packages, etc.).\"}\n",
    "      ]\n",
    "    ).choices[0].message.content\n",
    "    done = True\n",
    "    executor.shutdown(wait=True)\n",
    "    done = False\n",
    "    return list, have\n",
    "\n",
    "def modify_recipe_ai(client, ingredients, recipe, have, missing, executor):\n",
    "    \"\"\"Modifies the recipe to exclude missing ingredients using AI.\"\"\"\n",
    "    # check which ingredients user is missing\n",
    "    yes = ('y','Y','yes','Yes','YES')\n",
    "    no = ('n','N','no','No','NO')\n",
    "    for i in ingredients:\n",
    "        if i in have or i in missing:\n",
    "            continue\n",
    "        while True:\n",
    "            check = input(f\"Do you have {ingredients[i]} {i} (y/n)? \")\n",
    "            if check in yes:\n",
    "                have[i] = ingredients[i]\n",
    "                break\n",
    "            elif check in no:\n",
    "                missing[i] = ingredients[i]\n",
    "                break\n",
    "            print(\"Please enter 'y' or 'n'\")\n",
    "\n",
    "    # modify recipe\n",
    "    clear()\n",
    "    print(\"Generating modified recipe.\", end=\"\")\n",
    "    executor.submit(dots)\n",
    "    new_recipe = client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      response_format={\"type\": \"json_object\"},\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an accomodating chef, skilled at modifying recipes so they don't require certain ingredients, and designed to output the modified recipe in JSON.\"},\n",
    "        {\"role\": \"assistant\", \"content\": recipe},\n",
    "        {\"role\": \"user\", \"content\": f\"I like this recipe: {recipe} but I don't have these ingredients: {str(missing)}. Modify the recipe so it is similar but avoids or replaces the ingredients I don't have. {recipe_format}\"}\n",
    "      ]\n",
    "    ).choices[0].message.content\n",
    "    return json.loads(new_recipe), have, missing\n",
    "\n",
    "def new_recipe_ai(client, dish, experience, servings, allergies, recipe, missing, previous_dislikes, executor):\n",
    "    \"\"\"Fetches a new recipe based on dislikes of old recipe by name and experience level using AI and returns JSON.\"\"\"\n",
    "    current_dislikes = input(\"What do you dislike about the current recipe? \")\n",
    "    clear()\n",
    "    print(\"Generating new recipe.\", end=\"\")\n",
    "    executor.submit(dots)\n",
    "    new_recipe = client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      response_format={\"type\": \"json_object\"},\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a chef, skilled at creating and finding recipes for anyone and anything designed to output JSON.\"},\n",
    "        {\"role\": \"assistant\", \"content\": recipe},\n",
    "        {\"role\": \"user\", \"content\": f\"Create a new recipe for {dish} for a person of {experience} experience, knowing that these were their dislikes of the previous recipe: {current_dislikes}. They also had these dislikes of unknown older recipes: {previous_dislikes}. Include amounts of each ingredient for {servings} and avoid {allergies} due to allergies. Be sure to inlcude the name of the dish and the total time. {recipe_format}\"}\n",
    "      ]\n",
    "    ).choices[0].message.content\n",
    "    previous_dislikes.add(current_dislikes)\n",
    "    return json.loads(new_recipe), previous_dislikes\n",
    "\n",
    "def create_image_ai_steps(client, name, steps):\n",
    "    image = client.images.generate(\n",
    "      model=\"dall-e-3\",\n",
    "      prompt=f\"Create an image of a food called '{name}'. It is made with the following steps: {steps}. The image must be of the FINISHED food, as it is SERVED, not of the process. NO TEXT OR RAW INGREDIENTS IN THE IMAGE.\",\n",
    "      style=\"natural\",\n",
    "      size=\"1024x1024\",\n",
    "      quality=\"standard\",\n",
    "      response_format=\"b64_json\",\n",
    "      n=1,\n",
    "    )\n",
    "    return image.data[0].b64_json\n",
    "\n",
    "def show_image(image):    \n",
    "    image = image.split(\",\")[-1]\n",
    "    image_data = base64.b64decode(image)\n",
    "    image_stream = BytesIO(image_data)\n",
    "    display(Image(data=image_stream.read(), width=300, height=300))\n",
    "\n",
    "def print_recipe(recipe, servings):\n",
    "    name = recipe[\"name\"]\n",
    "    time = recipe[\"time\"]\n",
    "    ingredients = \"\"\n",
    "    for (ingredient, amount) in recipe[\"ingredients\"]:\n",
    "        ingredients += (f\"- {ingredient}, {amount}\\n\")\n",
    "    steps = \"\"\n",
    "    recipe_steps = recipe[\"steps\"]\n",
    "    for i in range(len(recipe_steps)):\n",
    "        steps += (f\"{i+1}. {recipe_steps[i]}\\n\")\n",
    "    print(f\"\\n{name}\\nTotal time: {time}\\nServings: {servings}\\n\\nIngredients:\\n{ingredients}\\nDirections:\\n{steps}\")\n",
    "\n",
    "def to_file(name, time, servings, ingredients, steps):\n",
    "    \"\"\"Prints recipe to file\"\"\"\n",
    "    # save printed recipe text\n",
    "    text = f\"\\n{name}\\nTotal time: {time}\\nServings: {servings}\\n\\nIngredients:\\n{ingredients}\\n\\nDirections:\\n{steps}\"\n",
    "    while True:\n",
    "        file_name = input(\"Enter file name to save the file to (must be a .txt; if it does not exist, the file will be created; if it does, it will be overwritten; make sure the file is not open): \")\n",
    "        try: # ensure file can open\n",
    "            f = open(file_name, \"w\")\n",
    "        except:\n",
    "            print(\"An error ocurred when opening this file. Please try again.\")\n",
    "            continue\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "        try: # ensure file can write\n",
    "            f.write(text)\n",
    "            f.close()\n",
    "        except:\n",
    "            print(\"An error ocurred when writing to this file. Please try again.\")\n",
    "        else:\n",
    "            break\n",
    "    return file_name\n",
    "\n",
    "def clear():\n",
    "    \"\"\"Displays the first few lines after clearing output\"\"\"\n",
    "    clear_output()\n",
    "    print(\"Welcome to the AI-powered Cooking Assistant!\\n\")\n",
    "\n",
    "def walkthrough_recipe(name, ingredients, steps):\n",
    "    \"\"\"Walks the user through the recipe step by step.\"\"\"\n",
    "    print(f\"Let's start cooking {name}!\")\n",
    "    print(\"First, get your ingredients together:\")\n",
    "    print(ingredients)\n",
    "    steps = steps.split(\"\\n\")\n",
    "    input(\"Press enter once you have all of your ingredients together.\")\n",
    "    print(\"\\nNow let's cook!\\nPress enter after completing each step to move on to the next.\")\n",
    "    for step in steps:\n",
    "        print(\"\\n\")\n",
    "        input(step)\n",
    "    print(\"\\nRecipe complete! Enjoy your meal!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06787e66-6dd1-476d-b6c2-c07f4c19068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_thread(client, file, speed):\n",
    "    \n",
    "    file = file.strip()\n",
    "    foods = scan_pantry(client, file, crop_limit=speed)\n",
    "    print(1)\n",
    "    for food in list(foods):\n",
    "        pantry.add(food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2dcdef8b-517f-42f8-bef6-19ae89c73389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dots():\n",
    "    global done\n",
    "    while not done:\n",
    "        start = timer.time()\n",
    "        while timer.time()-start<2:\n",
    "            pass\n",
    "        print('.', end='')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a2be5d90-9295-42b6-b1b0-1f0cbc0fb7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to run the cooking assistant\n",
    "pantry = set()\n",
    "done = False\n",
    "def cooking_assistant():\n",
    "    client = OpenAI()\n",
    "    missing = set()\n",
    "    have = set()\n",
    "    dislikes = set()\n",
    "    global pantry\n",
    "    global done\n",
    "    \n",
    "    print(\"Welcome to the AI-powered Cooking Assistant!\")\n",
    "\n",
    "    while True:\n",
    "        # User options\n",
    "        print(\"\\nDo you want to upload picture(s) of your pantry/fridge to scan for ingredients?\")\n",
    "        print(\"1. Yes - I will not be going to the store and need a recipe that uses my ingredients.\")\n",
    "        print(\"2. No - I will be going to the store anyway.\")\n",
    "        \n",
    "        choice = input(\"Enter your choice (1/2): \").strip()\n",
    "        \n",
    "        if choice == \"1\": # upload ingredient images\n",
    "            print(\"\\nLet's scan your pantry.\")\n",
    "            speed = 3\n",
    "\n",
    "            done = False\n",
    "            go = True\n",
    "            while go:\n",
    "                go = False\n",
    "                image_paths = input(\"Enter the path of each image of your pantry/fridge (comma separated): \").replace(\" \", \"\").split(',')\n",
    "                print('Reading and analyzing images.', end='')\n",
    "                executor = concurrent.futures.ThreadPoolExecutor(max_workers=2)\n",
    "                start = timer.time()\n",
    "                executor.submit(dots)\n",
    "                future = executor.submit(scan_pantry, client, image_paths, speed)\n",
    "            while not future.done():\n",
    "                pass\n",
    "            done = True\n",
    "            executor.shutdown()\n",
    "            done = False\n",
    "            print(future.result())\n",
    "            pantry = set(future.result())\n",
    "            end = timer.time()\n",
    "            with open('ingredients.txt', 'w') as f:\n",
    "                f.write(str(pantry))\n",
    "            print(f\"\\nImages read in {end-start} seconds.\")\n",
    "            break\n",
    "    \n",
    "        elif choice == \"2\": # upload and modify a recipe\n",
    "            print(\"\\nI'll keep your recipe open to any ingredients.\")\n",
    "            pantry = None\n",
    "            break\n",
    "    \n",
    "        else: # something other than 1 or 2 was entered - tell user and ask again\n",
    "            print(\"Invalid choice. Please enter only the number.\")\n",
    "\n",
    "    while True:\n",
    "        # User options\n",
    "        print(\"\\nWhat would you like to do?\")\n",
    "        print(\"1. Create a recipe.\")\n",
    "        print(\"2. Upload and modify my own recipe.\")\n",
    "        \n",
    "        choice = input(\"Enter your choice (1/2): \").strip()\n",
    "        executor = concurrent.futures.ThreadPoolExecutor(max_workers=2)\n",
    "        \n",
    "        if choice == \"1\": # create a recipe\n",
    "            print(\"\\nLet's create a recipe.\")\n",
    "            # Ask for user details\n",
    "            experience_level = input(\"What's your cooking experience level? \")\n",
    "            allergies = input(\"Do you have any allergies? (comma separated): \")\n",
    "            while True:\n",
    "                try:\n",
    "                    servings = int(input(\"How many servings are you cooking for? (number): \").strip())\n",
    "                except:\n",
    "                    print(\"That is not a number.\")\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            # Ask for desired dish\n",
    "            dish_name = input(\"What do you want to cook? (e.g., chocolate cupcakes, pasta, pastries, dinner): \")\n",
    "            print(\"\\nGenerating recipe.\", end=\"\")\n",
    "            done = False\n",
    "            executor.submit(dots)\n",
    "            recipe = ai_get_recipe_json(client, dish_name, experience_level, servings, allergies, pantry)\n",
    "            break\n",
    "    \n",
    "        elif choice == \"2\": # upload and modify a recipe\n",
    "            print(\"\\nLet's modify your recipe.\")\n",
    "            file_name = input(\"Ensure your recipe is in a .txt file. Enter the file path here: \")\n",
    "            while True:\n",
    "                try: # ensure file can open\n",
    "                    f = open(file_name, \"r\")\n",
    "                except:\n",
    "                    print(\"An error ocurred when opening this file. Please try again.\")\n",
    "                    continue\n",
    "                else:\n",
    "                    pass\n",
    "                    \n",
    "                try: # ensure file can write\n",
    "                    recipe = f.read()\n",
    "                except:\n",
    "                    print(\"An error ocurred when reading this file. Please try again.\")\n",
    "                else:\n",
    "                    break\n",
    "            print(\"File read successfully.\")\n",
    "            servings = input(\"How many servings are you cooking for (same or different from what the original recipe makes)? \")\n",
    "            print(\"Adjusting recipe.\", end=\"\")\n",
    "            executor.submit(dots)\n",
    "            if pantry:\n",
    "                recipe = convert_and_adjust_and_modify(client, recipe, servings, pantry)\n",
    "            else:\n",
    "                recipe = convert_and_adjust(client, recipe, servings)\n",
    "            dish_name = None\n",
    "            break\n",
    "    \n",
    "        else: # something other than 1 or 2 was entered - tell user and ask again\n",
    "            print(\"Invalid choice. Please enter only the number.\")\n",
    "\n",
    "    # Print recipe\n",
    "    print_recipe(client, recipe, servings)\n",
    "    if dish_name is None:\n",
    "        dish_name = recipe[\"name\"]\n",
    "\n",
    "    # Modify recipe until user likes it\n",
    "    while True:\n",
    "        # User options\n",
    "        print(\"\\nWhat would you like to do next?\")\n",
    "        print(\"1. I have all the ingredients - Start cooking!\")\n",
    "        print(\"2. Print this recipe.\")\n",
    "        print(\"3. I need to buy some ingredients - Create a grocery list.\")\n",
    "        print(\"4. I don't have some ingredients and can't buy them - Modify the recipe.\")\n",
    "        print(\"5. I don't like this recipe - Let's modify or make a new one.\")\n",
    "    \n",
    "        choice = input(\"Enter your choice (1/2/3/4/5): \").strip()\n",
    "    \n",
    "        if choice == \"1\": # start cooking\n",
    "            break\n",
    "\n",
    "        elif choice == \"2\": # print the recipe\n",
    "            print(\"\\nLet's print your recipe.\")\n",
    "            file_name = to_file(recipe[\"name\"], recipe[\"time\"], servings, recipe[\"ingredients\"], recipe[\"steps\"])\n",
    "            print(f\"Your recipe has been saved to {file_name}\")\n",
    "    \n",
    "        elif choice == \"3\": # create a grocery list\n",
    "            print(\"\\nLet's create your grocery list.\")\n",
    "            executor = concurrent.futures.ThreadPoolExecutor(max_workers=2)\n",
    "            list, have = create_grocery_list_ai(client, recipe[\"ingredients\"], have, executor)\n",
    "            print(list)\n",
    "            input(\"Press enter once you have purchased all of your ingredients\")\n",
    "    \n",
    "        elif choice == \"4\": # modify recipe\n",
    "            print(\"\\nLet's modify the recipe.\")\n",
    "            executor = concurrent.futures.ThreadPoolExecutor(max_workers=5)\n",
    "            recipe, have, missing = modify_recipe_ai(client, recipe[\"ingredients\"], recipe, have, missing, executor)\n",
    "            print_recipe(client, recipe, servings)\n",
    "            input(\"Press enter once you have reviewed the new recipe. If you are still missing ingredients, you can modify again.\")\n",
    "    \n",
    "        elif choice == \"5\": # new recipe\n",
    "            print(\"\\nLet's find another recipe.\")\n",
    "            executor = concurrent.futures.ThreadPoolExecutor(max_workers=5)\n",
    "            recipe = new_recipe_ai(client, dish_name, experience_level, servings, allergies, recipe, missing, executor)\n",
    "            print_recipe(client, recipe, servings, dislikes, executor)\n",
    "            input(\"Press enter once you have reviewed the new recipe. If you are still want to make changes, you can modify again.\")\n",
    "    \n",
    "        else: # something other than 1, 2, 3, 4, or 5 was entered - tell user and ask again\n",
    "            print(\"Invalid choice. Please enter only the number.\")\n",
    "\n",
    "    clear()\n",
    "    walkthrough_recipe(name, ingredients, steps)\n",
    "\n",
    "    # print recipe if necessary\n",
    "    yes = ('y','Y','yes','Yes','YES')\n",
    "    no = ('n','N','no','No','NO')\n",
    "    while True:\n",
    "        answer = input(\"In case you didn't print your recipe before, would you like to now (y/n)? \")\n",
    "        if answer in yes:\n",
    "            file_name = to_file(name, time, servings, ingredients, steps)\n",
    "            print(f\"Your recipe has been saved to {file_name}\")\n",
    "            break\n",
    "        elif answer in no:\n",
    "            break\n",
    "\n",
    "    # sign off\n",
    "    print(\"\\nNow that you're done cooking, I'm done for now! Come back next time you need help from the AI-powered Cooking Assistant!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9609325e-d39e-4997-a678-fb7d068d0752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the AI-powered Cooking Assistant!\n",
      "\n",
      "Do you want to upload picture(s) of your pantry/fridge to scan for ingredients?\n",
      "1. Yes - I will not be going to the store and need a recipe that uses my ingredients.\n",
      "2. No - I will be going to the store anyway.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice (1/2):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Let's scan your pantry.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the path of each image of your pantry/fridge (comma separated):  pantry4.JPG, pantry5.JPG, pantry6.JPG, pantry7.JPG, pantry8.JPG, pantry9.JPG, pantry10.JPG, fridge1.JPG, fridge2.JPG, fridge3.JPG, fridge4.JPG, fridge5.JPG, fridge6.JPG, fridge7.JPG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and analyzing images.................................................................\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#### Run the cooking assistant\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 3\u001b[0m     cooking_assistant()\n",
      "Cell \u001b[0;32mIn[49], line 41\u001b[0m, in \u001b[0;36mcooking_assistant\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m executor\u001b[38;5;241m.\u001b[39mshutdown()\n\u001b[1;32m     40\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28mprint\u001b[39m(future\u001b[38;5;241m.\u001b[39mresult())\n\u001b[1;32m     42\u001b[0m pantry \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(future\u001b[38;5;241m.\u001b[39mresult())\n\u001b[1;32m     43\u001b[0m end \u001b[38;5;241m=\u001b[39m timer\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "Cell \u001b[0;32mIn[39], line 30\u001b[0m, in \u001b[0;36mscan_pantry\u001b[0;34m(client, img_paths, crop_limit)\u001b[0m\n\u001b[1;32m     27\u001b[0m         crops\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# feed to chatgpt\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m foods \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(detect_food_items_gpt_mult(client, imgs)\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfood_items\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m foods\n",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m, in \u001b[0;36mdetect_food_items_gpt_mult\u001b[0;34m(client, imgs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m imgs:\n\u001b[1;32m      5\u001b[0m     msgs\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_url\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_url\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: img}})\n\u001b[0;32m----> 6\u001b[0m foods \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      7\u001b[0m   model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m   response_format\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson_object\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      9\u001b[0m   messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     10\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are an image analyzer, skilled at reading images of pantries and identifying every food item present, using object and text detection, and outputting that list in JSON.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     11\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: msgs    \n\u001b[1;32m     12\u001b[0m     }]\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m foods\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py:640\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    638\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    639\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    642\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    643\u001b[0m             {\n\u001b[1;32m    644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    645\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    646\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    647\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    648\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    649\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    650\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    651\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    652\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    653\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m    654\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    655\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    656\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    657\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m    658\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    659\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    660\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m    661\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    662\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    663\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    664\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    665\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    666\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    667\u001b[0m             },\n\u001b[1;32m    668\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    669\u001b[0m         ),\n\u001b[1;32m    670\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    671\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    672\u001b[0m         ),\n\u001b[1;32m    673\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    674\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    675\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    676\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1250\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1238\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1245\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1246\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1247\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1248\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1249\u001b[0m     )\n\u001b[0;32m-> 1250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:931\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    924\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    929\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    930\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    932\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    933\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    934\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    935\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    936\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m    937\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1015\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1014\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1015\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1016\u001b[0m         options,\n\u001b[1;32m   1017\u001b[0m         cast_to,\n\u001b[1;32m   1018\u001b[0m         retries,\n\u001b[1;32m   1019\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1020\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1021\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1022\u001b[0m     )\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1063\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1064\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1065\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1066\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m   1067\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1068\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1069\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1015\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1014\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1015\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1016\u001b[0m         options,\n\u001b[1;32m   1017\u001b[0m         cast_to,\n\u001b[1;32m   1018\u001b[0m         retries,\n\u001b[1;32m   1019\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1020\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1021\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1022\u001b[0m     )\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1063\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1064\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1065\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1066\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m   1067\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1068\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1069\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1030\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1027\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1029\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1030\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1033\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1034\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1037\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1038\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "#### Run the cooking assistant\n",
    "if __name__ == \"__main__\":\n",
    "    cooking_assistant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b870c4ee-4fb4-490c-94df-1cc5802eedd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
